#this is a notebook to review some basic concepts
9/16
Cost Function(Means squared error) = 1/nΣ(yi-(mxi+b))
#https://www.youtube.com/watch?v=vsWrXfO3wWw&list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw&index=4

Gradient Descenct : an algorithm that finds best fit line for given training data set
→ find the place of m&b where MSE is mininmum
like take derivative f'(x) = 0

9/17
from tensorflow website, explaining of the codes(https://www.tensorflow.org/text/tutorials/classify_text_with_bert)
(this code defines a text classification model that uses a pre-trained text preprocessing layer 
and a pre-trained text encoding layer (such as BERT) to process and encode input text data. 
The encoded data is then passed through a neural network with a dropout layer and a single-unit dense layer 
to produce a classification output.)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
#dropout layer with a drop out 0.1 is helpful to prevent overfitting training by randomly dropping a fraction of the output units
  net = tf.keras.layers.Dropout(0.1)(net)
#Dense layer acts as the classifier, producing a single scalar output. 
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net) 

#check the model's structure
tf.keras.utils.plot_model(classifier_model)

#in the binary classification problem, model outpus is a probability(a single-unit layer)
A loss function measures how good a neural network model is in performing a certain task, 
which in most cases is regression or classification.
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()

#fine-tuning section
Finetuning means taking weights of a trained neural network and use it as initialization for a new model being trained on data from the same domain (often e.g. images). 
It is used to:
  speed up the training
  overcome small dataset size
ファインチューニングの主要な部分は、BERTモデルの上に追加されたニューラルネットワークです。通常、BERTの事前学習済み部分（モデルの下層）は固定され、変更せず、新しいタスクに適した分類層や回帰層（出力層）が追加されます。この出力層はタスクに応じてカスタマイズされ、ファインチューニング対象となります。
ファインチューニングの一般的な手順は以下のようになります：
BERTの事前学習済みモデルをロード: 事前学習済みのBERTモデルをロードします。このモデルは通常、単語埋め込み（Word Embeddings）と位置埋め込み（Position Embeddings）から構成される下層モデルと、事前学習済みの重みが含まれています。
新しい出力層の追加: BERTの下層に、新しい出力層（分類層や回帰層）を追加します。この出力層はタスクに適したアーキテクチャにカスタマイズできます。通常、この出力層のパラメータ（重みとバイアス）はランダムに初期化されます。
ファインチューニング: ファインチューニングの際、通常は下層のBERTモデルの重みは固定され、新しい出力層のみがトレーニングされます。これにより、BERTの事前学習済み情報を保持しながら、新しいタスクに適応させることができます。
トレーニング: ファインチューニングの際、タスクに適した損失関数を使用して、モデルをトレーニングします。トレーニングデータセットを使用して、出力層のパラメータが最適な値に調整されます。
評価と微調整: ファインチューニングされたモデルは、評価データセットを使用して評価され、必要に応じて微調整されます。ファインチューニングの反復を繰り返すことがあるかもしれません
要するに、ファインチューニングの主な焦点は新しい出力層（分類層や回帰層）であり、BERTの下層は通常固定され、変更されません




  #optimizer(https://qiita.com/omiita/items/1735c1d048fe5f611f80,)
    An optimizer is a function or an algorithm that adjusts the attributes of the neural network, such as weights and learning rates. 
    Thus, it helps in reducing the overall loss and improving accuracy.
    (like trying gradient Descent multiple times to find best parameters, ADAM based on the Gradient Descent algorithm.)


The difference between neural network vs deep learning
  deep learning is the technology that has hidden layers more in the neural network.
  so, deep learning is the advanced one of neural network
  deep learning algorithms are "CNN","RNN","LSTM","RNN"
